---

title: PyEyesWeb

---

![Open Source](https://img.shields.io/badge/Open%20Source-Yes-brightgreen)
![Platform](https://img.shields.io/badge/Platform-Python-blue)
![Domain](https://img.shields.io/badge/Domain-Movement%20Analysis-purple)


<div class="hero" markdown>

# PyEyesWeb  
Movement Analysis Toolkit

:material-rocket-launch: **Extract features** from raw human movement data  
:material-school: **Apply in science, health, and the arts**  
:material-handshake: **Developed within the [Resilence EU Project](https://resilence.eu/)**

[:octicons-book-24: Get Started](getting_started.md){ .md-button .md-button--primary } [:material-github: GitHub Repository](https://github.com/InfoMusCP/InfoMove){ .md-button }  

</div>

---

## Why PyEyesWeb?

PyEyesWeb builds on the **Expressive Gesture Analysis** library of [EyesWeb](https://casapaganini.unige.it/eyesweb_bp){target="_blank"}, 
bringing expressive movement analysis into **Python** as a core aim of the project,
creating a modern, modular, and accessible toolkit for research, health, and artistic applications.  

PyEyesWeb is designed to facilitate adoption in **artificial intelligence and machine learning pipelines**,
while also enabling seamless integration with **creative and interactive platforms**
(e.g., **TouchDesigner, Unity, and Max/MSP** among others), supporting innovative,
cross-disciplinary projects at the intersection of science and the arts.

<details>
<summary>Learn more about EyesWeb</summary>

<a href="https://casapaganini.unige.it/eyesweb_bp" target="_blank">EyesWeb</a> is an open software research platform for the design and development of <b>real-time multimodal systems and interfaces</b>.  
It supports a wide variety of inputs, including motion capture, cameras, game controllers (Kinect, Wii), multichannel audio, and physiological signals.  
<br><br>
Outputs include multichannel audio, video, analog devices, and robotic platforms. EyesWeb provides libraries such as <b>Non-Verbal Expressive Gesture Analysis</b> and <b>Non-Verbal Social Signals Analysis</b>, and a visual programming environment that enables users to develop <b>real-time, networked applications</b>.  
<br><br>
Originally started in 1997, EyesWeb has been adopted worldwide in scientific research, education, and industry, including EU projects and collaborations with organizations such as INTEL and NYU.  

</details>

---

## Use Cases

<div class="grid cards" markdown>

-   :material-school:{ .lg .middle } **Research & Science**  

    ---

    Quantify movement expressivity and analyze biomechanics with validated methods.

-   :material-connection:{ .lg .middle } **Interactive Media**  

    ---

    Integrate PyEyesWeb in real-time with [TouchDesigner](extensions/touchdesigner.md).

-   :material-hospital:{ .lg .middle } **Health & Rehabilitation** 

    ---

    Assess movement disorders, monitor recovery, and support clinical studies.

-   :material-theater:{ .lg .middle } **Artistic Performance**  

    ---

    Explore synchrony, smoothness, and expressive qualities in dance and live performance.

</div>

---

## Methodological Foundation  

PyEyesWeb is informed by decades of research at the intersection of **movement science, computational modeling, and expressive gesture analysis**.  

A key methodological foundation of PyEyesWeb is the layered conceptual framework for analyzing expressive qualities of movement developed by [**InfoMus Lab â€“ Casa Paganini**](http://www.casapaganini.org/index_eng.php){:target="_blank"} [1].
This framework models an observer of a dance performance through four interconnected layers: from the physical signals captured by sensors, to the higher-level expressive qualities conveyed by movement (such as emotions).

!!! References
    [1] Camurri, A., Volpe, G., Piana, S., Mancini, M., Niewiadomski, R., Ferrari, N., & Canepa, C. (2016, July). The dancer in the eye: towards a multi-layered computational framework of qualities in movement. In Proceedings of the 3rd International Symposium on Movement and Computing (pp. 1-7).

---

## Project Context  

!!! info "About the Authors"
    PyEyesWeb is developed by [**InfoMus Lab â€“ Casa Paganini**](http://www.casapaganini.org/index_eng.php){:target="_blank"}, University of Genoa, as partners of the Resilence EU Project.  
    
    <div align="center">
    ![InfoMus Lab Logo](assets/cp-logo.png){ width=512 }
    </div>

PyEyesWeb is developed as part of the **[Resilence EU Project](https://www.resilence.eu/)**,  
funded by the European Unionâ€™s Horizon programme.  

<div align="center">

<img src="assets/resilence-logo.png" alt="Resilence Project Logo" width="200" style="margin:15px"/>
<img src="assets/eu-logo.png" alt="EU Logo" width="100" style="margin:15px"/>

</div>
---

## Explore the Documentation  

- ðŸš€ [Getting Started](getting-started.md) â€“ install, examples, and feature list
- ðŸŽ­ [TouchDesigner Plugin](integrations.md)

## Roadmap 

- Expand expressive feature extraction modules for movement analysis  
- Extend integrations with platforms like Unity and Max/MSP  
- Develop collaborative research tools and shared datasets

!!! info "Community & Collaboration"
    Whether you are a **researcher, artist, or developer**, PyEyesWeb helps you bridge movement, computation, and expression.  
    It is designed to be **modular, accessible, and integrable**, supporting a variety of use cases from scientific analysis to interactive artistic performances.

    ðŸ’¡ We welcome contributions, collaborations, and use cases from the community.  
    Check out our [GitHub repository](https://github.com/InfoMusCP/PyEyesWeb) to report issues, suggest features, or contribute code.

---

MIT Licensed Â· Open for collaboration  
